{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Seed\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv('../Data/train.csv').drop('ID', axis=1)\n",
    "test_df = pd.read_csv('../Data/test.csv').drop('ID', axis=1)\n",
    "submission_df = pd.read_csv('../Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          first_party                    second_party   \n",
       "0   Phil A. St. Amant              Herman A. Thompson  \\\n",
       "1      Stephen Duncan                  Lawrence Owens   \n",
       "2   Billy Joe Magwood  Tony Patterson, Warden, et al.   \n",
       "3          Linkletter                          Walker   \n",
       "4  William Earl Fikes                         Alabama   \n",
       "\n",
       "                                               facts  first_party_winner  \n",
       "0  On June 27, 1962, Phil St. Amant, a candidate ...                   1  \n",
       "1  Ramon Nelson was riding his bike when he suffe...                   0  \n",
       "2  An Alabama state court convicted Billy Joe Mag...                   1  \n",
       "3  Victor Linkletter was convicted in state court...                   0  \n",
       "4  On April 24, 1953 in Selma, Alabama, an intrud...                   1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phil A. St. Amant , Herman A. Thompson\n"
     ]
    }
   ],
   "source": [
    "fp_sample = train_df['first_party'][0]\n",
    "sp_sample = train_df['second_party'][0]\n",
    "print(fp_sample, ',', sp_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['first_party', 'second_party', 'facts']\n",
    "shortword = re.compile(r'\\W*\\b\\w{1}\\b') # 한글자 단어 제거\n",
    "tokenizer = TreebankWordTokenizer() # Treebank Tokenizer: 단어 단위로 토큰화\n",
    "stopword = stopwords.words('english') # NLTK 불용어 사전\n",
    "lemmatizer = WordNetLemmatizer() # NLTK 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def party_list(df, party_column):\n",
    "    party_list = []\n",
    "    for party in df[party_column]:\n",
    "        party = re.sub(shortword, '', party)\n",
    "        party = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", party)\n",
    "        token = tokenizer.tokenize(party)\n",
    "        party_token = [lemmatizer.lemmatize(word) for word in token if word not in stopword]\n",
    "        # 토큰으로 반환하면 party list로 만들 수 있지 않을까?\n",
    "        party_list.append(' '.join(party_token))\n",
    "    return party_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cleaning(df, column):\n",
    "    processed_list = []\n",
    "    for sen in df[column]:\n",
    "        sen = re.sub(shortword, '', sen)\n",
    "        sen = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", sen)\n",
    "        token = tokenizer.tokenize(sen)\n",
    "        sen_token = [lemmatizer.lemmatize(word) for word in token if word not in stopword]\n",
    "        processed_list.append(' '.join(sen_token))\n",
    "    return processed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_train = word_cleaning(train_df, 'first_party')\n",
    "sp_train = word_cleaning(train_df, 'second_party')\n",
    "facts_train = word_cleaning(train_df, 'facts')\n",
    "\n",
    "fp_test = word_cleaning(test_df, 'first_party')\n",
    "sp_test = word_cleaning(test_df, 'second_party')\n",
    "facts_test = word_cleaning(test_df, 'facts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_vectorizer(fp, sp, facts):\n",
    "    word_vec = CountVectorizer(ngram_range=(1, 2))\n",
    "    facts_vec = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    \n",
    "    word_vec.fit(fp + sp)\n",
    "    facts_vec.fit(facts)\n",
    "    \n",
    "    return word_vec, facts_vec\n",
    "\n",
    "def transform_vectorizer(fp, sp, facts, word_vec, facts_vec):\n",
    "    X1 = word_vec.transform(fp).toarray()\n",
    "    X2 = word_vec.transform(sp).toarray()\n",
    "    X3 = facts_vec.transform(facts).toarray()\n",
    "    \n",
    "    return np.concatenate((X1, X2, X3), axis=1)\n",
    "\n",
    "def word_vectorizer(fp_train, sp_train, facts_train, fp_test, sp_test, facts_test):\n",
    "    word_vec, facts_vec = fit_vectorizer(fp_train, sp_train, facts_train)\n",
    "    \n",
    "    X_train = transform_vectorizer(fp_train, sp_train, facts_train, word_vec, facts_vec)\n",
    "    X_test = transform_vectorizer(fp_test, sp_test, facts_test, word_vec, facts_vec)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = word_vectorizer(fp_train, sp_train, facts_train, fp_test, sp_test, facts_test)\n",
    "y_train = train_df['first_party_winner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2478, 204261) (2478,)\n",
      "(1240, 204261)\n",
      "first_party_winner\n",
      "1    1649\n",
      "0     829\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape after UnderSampling\n",
      "(1639, 204261) (1639,)\n",
      "====================\n",
      "Train target after UnderSampling\n",
      "first_party_winner\n",
      "0    829\n",
      "1    810\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불균형 문제 전처리(언더샘플링)\n",
    "X_nc, y_nc = NeighbourhoodCleaningRule(n_neighbors=3).fit_resample(X_train, y_train)\n",
    "print('Train Data Shape after UnderSampling')\n",
    "print(X_nc.shape, y_nc.shape)\n",
    "print('='*20)\n",
    "print('Train target after UnderSampling')\n",
    "print(y_nc.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape\n",
      "(1229, 204261) (1229,)\n",
      "--------------------\n",
      "Train target\n",
      "first_party_winner\n",
      "0    622\n",
      "1    607\n",
      "Name: count, dtype: int64\n",
      "====================\n",
      "Validation Data Shape\n",
      "(410, 204261) (410,)\n",
      "--------------------\n",
      "Validation target\n",
      "first_party_winner\n",
      "0    207\n",
      "1    203\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Train, Validation 분리\n",
    "Train_X, Val_X, Train_y, Val_y = train_test_split(X_nc, y_nc, test_size=.25, random_state=42, stratify=y_nc)\n",
    "print('Train Data Shape')\n",
    "print(Train_X.shape, Train_y.shape)\n",
    "print('-'*20)\n",
    "print('Train target')\n",
    "print(Train_y.value_counts())\n",
    "print('='*20)\n",
    "print('Validation Data Shape')\n",
    "print(Val_X.shape, Val_y.shape)\n",
    "print('-'*20)\n",
    "print('Validation target')\n",
    "print(Val_y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.69       207\n",
      "           1       0.69      0.65      0.67       203\n",
      "\n",
      "    accuracy                           0.68       410\n",
      "   macro avg       0.68      0.68      0.68       410\n",
      "weighted avg       0.68      0.68      0.68       410\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Logistic = LogisticRegression(max_iter=500, random_state=42)\n",
    "Logistic.fit(Train_X, Train_y)\n",
    "print(classification_report(Val_y, Logistic.predict(Val_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['first_party_winner'] = Logistic.predict(X_test)\n",
    "submission_df.to_csv('logi___2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
