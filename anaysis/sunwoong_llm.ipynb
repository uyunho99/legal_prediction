{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEA\n",
    "\n",
    "1. fp, sp 인코딩\n",
    "    - fp, sp가 직접적으로 나오는 full-name, 약자 모두 인코딩\n",
    "    - 대명사 => X\n",
    "    - 대문자 => 소문자 X\n",
    "    - fp: FisrtParty, sp: SecondParty  \n",
    "        => bert base tokenizer에서 fp, sp 또는 그냥 이름들이 OOV 처리를 받는지 확인  \n",
    "        \n",
    "===== Augmentation =====  \n",
    "1. fp, sp -> sp, fp 바꿔서 label 반전\n",
    "2. 문장내 fp, sp를 랜덤하게 마스킹\n",
    "3. 문장내 fp, sp를 랜덤하게 remove ???\n",
    "\n",
    "===== Hyphothesis =====  \n",
    "1. fp, sp를 다른 이름으로 바꿈 => label: 2 : padon\n",
    "2. facts, Q1: who is fp?, Q2: who is sp?, Q3: who won? (fp, sp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 13:16:37.880736: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-29 13:16:37.941899: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-29 13:16:37.943893: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bluesun/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/dan/anaconda3/lib/:/home/dan/anaconda3/lib/python3.8/site-packages/tensorrt/:/home/dan/anaconda3/envs/tf/lib:/home/dan/anaconda3/envs/tf/lib/python3.8/site-packages/tensorrt/\n",
      "2023-06-29 13:16:37.943900: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-29 13:16:38.278821: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bluesun/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/dan/anaconda3/lib/:/home/dan/anaconda3/lib/python3.8/site-packages/tensorrt/:/home/dan/anaconda3/envs/tf/lib:/home/dan/anaconda3/envs/tf/lib/python3.8/site-packages/tensorrt/\n",
      "2023-06-29 13:16:38.278862: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bluesun/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/dan/anaconda3/lib/:/home/dan/anaconda3/lib/python3.8/site-packages/tensorrt/:/home/dan/anaconda3/envs/tf/lib:/home/dan/anaconda3/envs/tf/lib/python3.8/site-packages/tensorrt/\n",
      "2023-06-29 13:16:38.278866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./Data/train.csv')\n",
    "test_df = pd.read_csv('./Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_rename = {'first_party': 'fp',\n",
    "                 'second_party': 'sp',\n",
    "                 'first_party_winner': 'label'}\n",
    "\n",
    "train_df.rename(columns=column_rename, inplace=True)\n",
    "test_df.rename(columns=column_rename, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>fp</th>\n",
       "      <th>sp</th>\n",
       "      <th>facts</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>TRAIN_2473</td>\n",
       "      <td>HollyFrontier Cheyenne Refining, LLC, et al.</td>\n",
       "      <td>Renewable Fuels Association, et al.</td>\n",
       "      <td>Congress amended the Clean Air Act through the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>TRAIN_2474</td>\n",
       "      <td>Grupo Mexicano de Desarrollo, S. A.</td>\n",
       "      <td>Alliance Bond Fund, Inc.</td>\n",
       "      <td>Alliance Bond Fund, Inc., an investment fund, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>TRAIN_2475</td>\n",
       "      <td>Peguero</td>\n",
       "      <td>United States</td>\n",
       "      <td>In 1992, the District Court sentenced Manuel D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>TRAIN_2476</td>\n",
       "      <td>Immigration and Naturalization Service</td>\n",
       "      <td>St. Cyr</td>\n",
       "      <td>On March 8, 1996, Enrico St. Cyr, a lawful per...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>TRAIN_2477</td>\n",
       "      <td>Markman</td>\n",
       "      <td>Westview Instruments, Inc.</td>\n",
       "      <td>Herbert Markman owns the patent to a system th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2478 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                            fp  \\\n",
       "0     TRAIN_0000                             Phil A. St. Amant   \n",
       "1     TRAIN_0001                                Stephen Duncan   \n",
       "2     TRAIN_0002                             Billy Joe Magwood   \n",
       "3     TRAIN_0003                                    Linkletter   \n",
       "4     TRAIN_0004                            William Earl Fikes   \n",
       "...          ...                                           ...   \n",
       "2473  TRAIN_2473  HollyFrontier Cheyenne Refining, LLC, et al.   \n",
       "2474  TRAIN_2474           Grupo Mexicano de Desarrollo, S. A.   \n",
       "2475  TRAIN_2475                                       Peguero   \n",
       "2476  TRAIN_2476        Immigration and Naturalization Service   \n",
       "2477  TRAIN_2477                                       Markman   \n",
       "\n",
       "                                       sp  \\\n",
       "0                      Herman A. Thompson   \n",
       "1                          Lawrence Owens   \n",
       "2          Tony Patterson, Warden, et al.   \n",
       "3                                  Walker   \n",
       "4                                 Alabama   \n",
       "...                                   ...   \n",
       "2473  Renewable Fuels Association, et al.   \n",
       "2474             Alliance Bond Fund, Inc.   \n",
       "2475                        United States   \n",
       "2476                              St. Cyr   \n",
       "2477           Westview Instruments, Inc.   \n",
       "\n",
       "                                                  facts  label  \n",
       "0     On June 27, 1962, Phil St. Amant, a candidate ...      1  \n",
       "1     Ramon Nelson was riding his bike when he suffe...      0  \n",
       "2     An Alabama state court convicted Billy Joe Mag...      1  \n",
       "3     Victor Linkletter was convicted in state court...      0  \n",
       "4     On April 24, 1953 in Selma, Alabama, an intrud...      1  \n",
       "...                                                 ...    ...  \n",
       "2473  Congress amended the Clean Air Act through the...      1  \n",
       "2474  Alliance Bond Fund, Inc., an investment fund, ...      1  \n",
       "2475  In 1992, the District Court sentenced Manuel D...      0  \n",
       "2476  On March 8, 1996, Enrico St. Cyr, a lawful per...      0  \n",
       "2477  Herbert Markman owns the patent to a system th...      0  \n",
       "\n",
       "[2478 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1649\n",
       "0     829\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "train_df, _ = ros.fit_resample(train_df, train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1649\n",
       "0    1649\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>fp</th>\n",
       "      <th>sp</th>\n",
       "      <th>facts</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293</th>\n",
       "      <td>TRAIN_2248</td>\n",
       "      <td>Estelle</td>\n",
       "      <td>Rummel</td>\n",
       "      <td>After being convicted of three felonies over a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>TRAIN_1654</td>\n",
       "      <td>District Director, Immigration and Naturalizat...</td>\n",
       "      <td>Kalman J. Berenyi</td>\n",
       "      <td>Kalman Berenyi applied for naturalization as a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3295</th>\n",
       "      <td>TRAIN_1617</td>\n",
       "      <td>United States</td>\n",
       "      <td>Dow Chemical Company</td>\n",
       "      <td>Dow Chemical Company denied the Environmental ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3296</th>\n",
       "      <td>TRAIN_0743</td>\n",
       "      <td>Chakrabarty</td>\n",
       "      <td>Diamond</td>\n",
       "      <td>After genetically engineering a bacterium capa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3297</th>\n",
       "      <td>TRAIN_1185</td>\n",
       "      <td>United States, ex rel. Billy Joe Hunt</td>\n",
       "      <td>Cochise Consultancy, Inc. et al.</td>\n",
       "      <td>The US Department of Defense awarded petitione...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3298 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                                 fp  \\\n",
       "0     TRAIN_0000                                 Herman A. Thompson   \n",
       "1     TRAIN_0001                                     Lawrence Owens   \n",
       "2     TRAIN_0002                     Tony Patterson, Warden, et al.   \n",
       "3     TRAIN_0003                                             Walker   \n",
       "4     TRAIN_0004                                            Alabama   \n",
       "...          ...                                                ...   \n",
       "3293  TRAIN_2248                                            Estelle   \n",
       "3294  TRAIN_1654  District Director, Immigration and Naturalizat...   \n",
       "3295  TRAIN_1617                                      United States   \n",
       "3296  TRAIN_0743                                        Chakrabarty   \n",
       "3297  TRAIN_1185              United States, ex rel. Billy Joe Hunt   \n",
       "\n",
       "                                    sp  \\\n",
       "0                    Phil A. St. Amant   \n",
       "1                       Stephen Duncan   \n",
       "2                    Billy Joe Magwood   \n",
       "3                           Linkletter   \n",
       "4                   William Earl Fikes   \n",
       "...                                ...   \n",
       "3293                            Rummel   \n",
       "3294                Kalman J. Berenyi    \n",
       "3295              Dow Chemical Company   \n",
       "3296                           Diamond   \n",
       "3297  Cochise Consultancy, Inc. et al.   \n",
       "\n",
       "                                                  facts  label  \n",
       "0     On June 27, 1962, Phil St. Amant, a candidate ...      0  \n",
       "1     Ramon Nelson was riding his bike when he suffe...      1  \n",
       "2     An Alabama state court convicted Billy Joe Mag...      0  \n",
       "3     Victor Linkletter was convicted in state court...      1  \n",
       "4     On April 24, 1953 in Selma, Alabama, an intrud...      0  \n",
       "...                                                 ...    ...  \n",
       "3293  After being convicted of three felonies over a...      1  \n",
       "3294  Kalman Berenyi applied for naturalization as a...      1  \n",
       "3295  Dow Chemical Company denied the Environmental ...      1  \n",
       "3296  After genetically engineering a bacterium capa...      1  \n",
       "3297  The US Department of Defense awarded petitione...      1  \n",
       "\n",
       "[3298 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_df = pd.DataFrame({'ID': train_df['ID'],\n",
    "                       'fp': train_df['sp'],\n",
    "                       'sp': train_df['fp'],\n",
    "                       'facts': train_df['facts'],\n",
    "                       'label': 1-train_df['label']})\n",
    "\n",
    "aug_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>fp</th>\n",
       "      <th>sp</th>\n",
       "      <th>facts</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6591</th>\n",
       "      <td>TRAIN_2248</td>\n",
       "      <td>Estelle</td>\n",
       "      <td>Rummel</td>\n",
       "      <td>After being convicted of three felonies over a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6592</th>\n",
       "      <td>TRAIN_1654</td>\n",
       "      <td>District Director, Immigration and Naturalizat...</td>\n",
       "      <td>Kalman J. Berenyi</td>\n",
       "      <td>Kalman Berenyi applied for naturalization as a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6593</th>\n",
       "      <td>TRAIN_1617</td>\n",
       "      <td>United States</td>\n",
       "      <td>Dow Chemical Company</td>\n",
       "      <td>Dow Chemical Company denied the Environmental ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6594</th>\n",
       "      <td>TRAIN_0743</td>\n",
       "      <td>Chakrabarty</td>\n",
       "      <td>Diamond</td>\n",
       "      <td>After genetically engineering a bacterium capa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6595</th>\n",
       "      <td>TRAIN_1185</td>\n",
       "      <td>United States, ex rel. Billy Joe Hunt</td>\n",
       "      <td>Cochise Consultancy, Inc. et al.</td>\n",
       "      <td>The US Department of Defense awarded petitione...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6596 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                                 fp  \\\n",
       "0     TRAIN_0000                                  Phil A. St. Amant   \n",
       "1     TRAIN_0001                                     Stephen Duncan   \n",
       "2     TRAIN_0002                                  Billy Joe Magwood   \n",
       "3     TRAIN_0003                                         Linkletter   \n",
       "4     TRAIN_0004                                 William Earl Fikes   \n",
       "...          ...                                                ...   \n",
       "6591  TRAIN_2248                                            Estelle   \n",
       "6592  TRAIN_1654  District Director, Immigration and Naturalizat...   \n",
       "6593  TRAIN_1617                                      United States   \n",
       "6594  TRAIN_0743                                        Chakrabarty   \n",
       "6595  TRAIN_1185              United States, ex rel. Billy Joe Hunt   \n",
       "\n",
       "                                    sp  \\\n",
       "0                   Herman A. Thompson   \n",
       "1                       Lawrence Owens   \n",
       "2       Tony Patterson, Warden, et al.   \n",
       "3                               Walker   \n",
       "4                              Alabama   \n",
       "...                                ...   \n",
       "6591                            Rummel   \n",
       "6592                Kalman J. Berenyi    \n",
       "6593              Dow Chemical Company   \n",
       "6594                           Diamond   \n",
       "6595  Cochise Consultancy, Inc. et al.   \n",
       "\n",
       "                                                  facts  label  \n",
       "0     On June 27, 1962, Phil St. Amant, a candidate ...      1  \n",
       "1     Ramon Nelson was riding his bike when he suffe...      0  \n",
       "2     An Alabama state court convicted Billy Joe Mag...      1  \n",
       "3     Victor Linkletter was convicted in state court...      0  \n",
       "4     On April 24, 1953 in Selma, Alabama, an intrud...      1  \n",
       "...                                                 ...    ...  \n",
       "6591  After being convicted of three felonies over a...      1  \n",
       "6592  Kalman Berenyi applied for naturalization as a...      1  \n",
       "6593  Dow Chemical Company denied the Environmental ...      1  \n",
       "6594  After genetically engineering a bacterium capa...      1  \n",
       "6595  The US Department of Defense awarded petitione...      1  \n",
       "\n",
       "[6596 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([train_df, aug_df], ignore_index=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID       0\n",
       "fp       0\n",
       "sp       0\n",
       "facts    0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type casting\n",
    "# train_df['label'] = train_df['label'].astype(str)\n",
    "# train_df['label'] = train_df['label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = Dataset.from_pandas(pd.read_csv('./Data/train.csv'))\n",
    "# test_data = Dataset.from_pandas(pd.read_csv('./Data/test.csv'))\n",
    "\n",
    "train_data = Dataset.from_pandas(train_df)\n",
    "test_data = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': Value(dtype='string', id=None),\n",
       " 'fp': Value(dtype='string', id=None),\n",
       " 'sp': Value(dtype='string', id=None),\n",
       " 'facts': Value(dtype='string', id=None),\n",
       " 'label': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "label = datasets.ClassLabel(num_classes=2, names=[0, 1])\n",
    "train_data.features['label'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': Value(dtype='string', id=None),\n",
       " 'fp': Value(dtype='string', id=None),\n",
       " 'sp': Value(dtype='string', id=None),\n",
       " 'facts': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=[0, 1], id=None)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'fp', 'sp', 'facts', 'label'],\n",
       "    num_rows: 6596\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'fp', 'sp', 'facts', 'label'],\n",
       "        num_rows: 5276\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ID', 'fp', 'sp', 'facts', 'label'],\n",
       "        num_rows: 1320\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'fp', 'sp', 'facts'],\n",
       "        num_rows: 1240\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.train_test_split(test_size=0.2, shuffle=True, seed=42, stratify_by_column='label')\n",
    "dataset = DatasetDict({'train': train_data['train'], 'validation': train_data['test'], 'test': test_data})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(train_data['train']['label']), np.mean(train_data['test']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model = \"distilbert-base-uncased\"\n",
    "pretrained_model = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234ea723316040e7b75d50cfcdaf7977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1982 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47408fda864340079e4f023516061a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48394c05797b4b3aa6760551b72e8cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(examples):\n",
    "    examples['facts_with_party'] = [f'First: {first}.\\n Second: {second}.\\nFacts: {fact}' \n",
    "                                    for first, second, fact in zip(examples['first_party'], examples['second_party'], examples['facts'])]\n",
    "    return tokenizer(examples['facts_with_party'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "dataset = dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'first_party', 'second_party', 'facts', 'label', 'facts_with_party', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1982\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ID', 'first_party', 'second_party', 'facts', 'label', 'facts_with_party', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 496\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'first_party', 'second_party', 'facts', 'facts_with_party', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1240\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing DistilBertModel: ['roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'lm_head.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'lm_head.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.dense.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.weight', 'embeddings.position_embeddings.weight', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.5.attention.out_lin.bias', 'embeddings.LayerNorm.weight', 'transformer.layer.11.output_layer_norm.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.9.attention.q_lin.bias', 'embeddings.word_embeddings.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.3.ffn.lin2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel\n",
    "\n",
    "model = DistilBertModel.from_pretrained(pretrained_model, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: second_party, first_party, ID, facts, facts_with_party. If second_party, first_party, ID, facts, facts_with_party are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/bluesun/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1982\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 620\n",
      "  Number of trainable parameters = 66955010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e78390ee9594bcc838798dc8476b5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: second_party, first_party, ID, facts, facts_with_party. If second_party, first_party, ID, facts, facts_with_party are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 496\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513b868c3ad4489fa1798e3606260e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-124\n",
      "Configuration saved in ./results/checkpoint-124/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6590800881385803, 'eval_accuracy': 0.6431451612903226, 'eval_runtime': 2.2053, 'eval_samples_per_second': 224.916, 'eval_steps_per_second': 14.057, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-124/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: second_party, first_party, ID, facts, facts_with_party. If second_party, first_party, ID, facts, facts_with_party are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 496\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c03806a64241318447531ceb25002d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-248\n",
      "Configuration saved in ./results/checkpoint-248/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6946614384651184, 'eval_accuracy': 0.6471774193548387, 'eval_runtime': 2.2537, 'eval_samples_per_second': 220.079, 'eval_steps_per_second': 13.755, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-248/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: second_party, first_party, ID, facts, facts_with_party. If second_party, first_party, ID, facts, facts_with_party are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 496\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3be330895b4bcdb267c577008186c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-372\n",
      "Configuration saved in ./results/checkpoint-372/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7725196480751038, 'eval_accuracy': 0.5665322580645161, 'eval_runtime': 2.2733, 'eval_samples_per_second': 218.184, 'eval_steps_per_second': 13.636, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-372/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: second_party, first_party, ID, facts, facts_with_party. If second_party, first_party, ID, facts, facts_with_party are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 496\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993bc20fe00047b9acc18da38225f8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-496\n",
      "Configuration saved in ./results/checkpoint-496/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0630594491958618, 'eval_accuracy': 0.5846774193548387, 'eval_runtime': 2.2655, 'eval_samples_per_second': 218.94, 'eval_steps_per_second': 13.684, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-496/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5393, 'learning_rate': 3.870967741935484e-06, 'epoch': 4.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: second_party, first_party, ID, facts, facts_with_party. If second_party, first_party, ID, facts, facts_with_party are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 496\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b347ba5bbd4420bea787b81436e894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-620\n",
      "Configuration saved in ./results/checkpoint-620/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1904042959213257, 'eval_accuracy': 0.5725806451612904, 'eval_runtime': 2.2313, 'eval_samples_per_second': 222.294, 'eval_steps_per_second': 13.893, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-620/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-248 (score: 0.6471774193548387).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 145.2678, 'train_samples_per_second': 68.219, 'train_steps_per_second': 4.268, 'train_loss': 0.48051954084827053, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=620, training_loss=0.48051954084827053, metrics={'train_runtime': 145.2678, 'train_samples_per_second': 68.219, 'train_steps_per_second': 4.268, 'train_loss': 0.48051954084827053, 'epoch': 5.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: second_party, first_party, ID, facts, facts_with_party. If second_party, first_party, ID, facts, facts_with_party are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 496\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e049d0e5d4cb4c5bb4e0167b783d7fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "with th.no_grad():\n",
    "    logits = trainer.predict(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(logits.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(dataset['validation']['label']) - logits.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.6946614384651184,\n",
       " 'test_accuracy': 0.6471774193548387,\n",
       " 'test_runtime': 2.2437,\n",
       " 'test_samples_per_second': 221.067,\n",
       " 'test_steps_per_second': 13.817}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: second_party, first_party, ID, facts, facts_with_party. If second_party, first_party, ID, facts, facts_with_party are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1240\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8e37da3ac24830a7e7d2217b57ca06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with th.no_grad():\n",
    "    logits = trainer.predict(dataset['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 0.03335458,  0.0423288 ],\n",
       "       [-0.23478237,  0.32663944],\n",
       "       [ 0.16238324, -0.07501629],\n",
       "       ...,\n",
       "       [-0.4196741 ,  0.53455144],\n",
       "       [-0.13286527,  0.15887444],\n",
       "       [-0.8414401 ,  0.93614745]], dtype=float32), label_ids=None, metrics={'test_runtime': 5.5602, 'test_samples_per_second': 223.015, 'test_steps_per_second': 14.028})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logits.predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9612903225806452"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'first_party', 'second_party', 'facts', 'facts_with_party', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1240\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>TEST_1235</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>TEST_1236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>TEST_1237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>TEST_1238</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>TEST_1239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  first_party_winner\n",
       "0     TEST_0000                   1\n",
       "1     TEST_0001                   1\n",
       "2     TEST_0002                   0\n",
       "3     TEST_0003                   1\n",
       "4     TEST_0004                   1\n",
       "...         ...                 ...\n",
       "1235  TEST_1235                   1\n",
       "1236  TEST_1236                   1\n",
       "1237  TEST_1237                   1\n",
       "1238  TEST_1238                   1\n",
       "1239  TEST_1239                   1\n",
       "\n",
       "[1240 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'ID': dataset['test']['ID'], 'first_party_winner': preds})\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "def to_input(batch):\n",
    "    input_ids = th.tensor(batch['input_ids'])\n",
    "    attention_mask = th.tensor(batch['attention_mask'])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "input_data = to_input(dataset['test'][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minput_data)[\u001b[39m'\u001b[39m\u001b[39mlast_hidden_state\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model(**input_data)['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_data = to_input(dataset['train'])\n",
    "test_input_data = to_input(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m코드를 실행할 수 없습니다. 세션이 삭제되었습니다. 커널을 다시 시작해 보세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "train_emb = model(**train_input_data)['last_hidden_state'][:, 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m코드를 실행할 수 없습니다. 세션이 삭제되었습니다. 커널을 다시 시작해 보세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "test_emb = model(**test_input_data)['last_hidden_state'][:, 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(\u001b[39m'\u001b[39;49m\u001b[39mABC\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:563\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'size'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
