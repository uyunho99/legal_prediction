{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yunho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/yunho/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Seed\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv('../Data/train.csv').drop('ID', axis=1)\n",
    "test_df = pd.read_csv('../Data/test.csv').drop('ID', axis=1)\n",
    "submission_df = pd.read_csv('../Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          first_party                    second_party   \n",
       "0   Phil A. St. Amant              Herman A. Thompson  \\\n",
       "1      Stephen Duncan                  Lawrence Owens   \n",
       "2   Billy Joe Magwood  Tony Patterson, Warden, et al.   \n",
       "3          Linkletter                          Walker   \n",
       "4  William Earl Fikes                         Alabama   \n",
       "\n",
       "                                               facts  first_party_winner  \n",
       "0  On June 27, 1962, Phil St. Amant, a candidate ...                   1  \n",
       "1  Ramon Nelson was riding his bike when he suffe...                   0  \n",
       "2  An Alabama state court convicted Billy Joe Mag...                   1  \n",
       "3  Victor Linkletter was convicted in state court...                   0  \n",
       "4  On April 24, 1953 in Selma, Alabama, an intrud...                   1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4951</th>\n",
       "      <td>Renewable Fuels Association, et al.</td>\n",
       "      <td>HollyFrontier Cheyenne Refining, LLC, et al.</td>\n",
       "      <td>Congress amended the Clean Air Act through the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4952</th>\n",
       "      <td>Alliance Bond Fund, Inc.</td>\n",
       "      <td>Grupo Mexicano de Desarrollo, S. A.</td>\n",
       "      <td>Alliance Bond Fund, Inc., an investment fund, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>United States</td>\n",
       "      <td>Peguero</td>\n",
       "      <td>In 1992, the District Court sentenced Manuel D...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4954</th>\n",
       "      <td>St. Cyr</td>\n",
       "      <td>Immigration and Naturalization Service</td>\n",
       "      <td>On March 8, 1996, Enrico St. Cyr, a lawful per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>Westview Instruments, Inc.</td>\n",
       "      <td>Markman</td>\n",
       "      <td>Herbert Markman owns the patent to a system th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4956 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              first_party   \n",
       "0                       Phil A. St. Amant  \\\n",
       "1                          Stephen Duncan   \n",
       "2                       Billy Joe Magwood   \n",
       "3                              Linkletter   \n",
       "4                      William Earl Fikes   \n",
       "...                                   ...   \n",
       "4951  Renewable Fuels Association, et al.   \n",
       "4952             Alliance Bond Fund, Inc.   \n",
       "4953                        United States   \n",
       "4954                              St. Cyr   \n",
       "4955           Westview Instruments, Inc.   \n",
       "\n",
       "                                      second_party   \n",
       "0                               Herman A. Thompson  \\\n",
       "1                                   Lawrence Owens   \n",
       "2                   Tony Patterson, Warden, et al.   \n",
       "3                                           Walker   \n",
       "4                                          Alabama   \n",
       "...                                            ...   \n",
       "4951  HollyFrontier Cheyenne Refining, LLC, et al.   \n",
       "4952           Grupo Mexicano de Desarrollo, S. A.   \n",
       "4953                                       Peguero   \n",
       "4954        Immigration and Naturalization Service   \n",
       "4955                                       Markman   \n",
       "\n",
       "                                                  facts  first_party_winner  \n",
       "0     On June 27, 1962, Phil St. Amant, a candidate ...                   1  \n",
       "1     Ramon Nelson was riding his bike when he suffe...                   0  \n",
       "2     An Alabama state court convicted Billy Joe Mag...                   1  \n",
       "3     Victor Linkletter was convicted in state court...                   0  \n",
       "4     On April 24, 1953 in Selma, Alabama, an intrud...                   1  \n",
       "...                                                 ...                 ...  \n",
       "4951  Congress amended the Clean Air Act through the...                   0  \n",
       "4952  Alliance Bond Fund, Inc., an investment fund, ...                   0  \n",
       "4953  In 1992, the District Court sentenced Manuel D...                   1  \n",
       "4954  On March 8, 1996, Enrico St. Cyr, a lawful per...                   1  \n",
       "4955  Herbert Markman owns the patent to a system th...                   1  \n",
       "\n",
       "[4956 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_df = pd.DataFrame({'first_party': train_df['second_party'],\n",
    "                       'second_party': train_df['first_party'],\n",
    "                       'facts': train_df['facts'],\n",
    "                       'first_party_winner': 1-train_df['first_party_winner']})\n",
    "\n",
    "train_df = pd.concat([train_df, aug_df], ignore_index=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phil A. St. Amant , Herman A. Thompson\n"
     ]
    }
   ],
   "source": [
    "fp_sample = train_df['first_party'][0]\n",
    "sp_sample = train_df['second_party'][0]\n",
    "print(fp_sample, ',', sp_sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortword = re.compile(r'\\W*\\b\\w{1}\\b') # 한글자 단어 제거\n",
    "tokenizer = TreebankWordTokenizer() # Treebank Tokenizer: 단어 단위로 토큰화\n",
    "stopword = stopwords.words('english') # NLTK 불용어 사전\n",
    "lemmatizer = WordNetLemmatizer() # NLTK 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본 함수는 사용하지 않지만, 나중에 party list를 만드는데 유용할 것 같아서 남겨둠\n",
    "def party_list(df, party_column):\n",
    "    party_list = []\n",
    "    for party in df[party_column]:\n",
    "        party = re.sub(shortword, '', party)\n",
    "        party = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", party)\n",
    "        token = tokenizer.tokenize(party)\n",
    "        party_token = [lemmatizer.lemmatize(word) for word in token if word not in stopword]\n",
    "        # 토큰으로 반환하면 party list로 만들 수 있지 않을까?\n",
    "        party_list.append(' '.join(party_token))\n",
    "    return party_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cleaning(df, column):\n",
    "    processed_list = []\n",
    "    for sen in df[column]:\n",
    "        sen = re.sub(shortword, '', sen)\n",
    "        sen = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", sen)\n",
    "        token = tokenizer.tokenize(sen)\n",
    "        sen_token = [lemmatizer.lemmatize(word) for word in token if word not in stopword]\n",
    "        processed_list.append(' '.join(sen_token))\n",
    "    return processed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_train = word_cleaning(train_df, 'first_party')\n",
    "sp_train = word_cleaning(train_df, 'second_party')\n",
    "facts_train = word_cleaning(train_df, 'facts')\n",
    "\n",
    "fp_test = word_cleaning(test_df, 'first_party')\n",
    "sp_test = word_cleaning(test_df, 'second_party')\n",
    "facts_test = word_cleaning(test_df, 'facts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_vectorizer(fp, sp, facts):\n",
    "    word_vec = CountVectorizer(ngram_range=(1, 2))\n",
    "    facts_vec = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    \n",
    "    word_vec.fit(fp + sp)\n",
    "    facts_vec.fit(facts)\n",
    "    \n",
    "    return word_vec, facts_vec\n",
    "\n",
    "def transform_vectorizer(fp, sp, facts, word_vec, facts_vec):\n",
    "    X1 = word_vec.transform(fp).toarray()\n",
    "    X2 = word_vec.transform(sp).toarray()\n",
    "    X3 = facts_vec.transform(facts).toarray()\n",
    "    \n",
    "    return np.concatenate((X1, X2, X3), axis=1)\n",
    "\n",
    "def word_vectorizer(fp_train, sp_train, facts_train, fp_test, sp_test, facts_test):\n",
    "    word_vec, facts_vec = fit_vectorizer(fp_train, sp_train, facts_train)\n",
    "    \n",
    "    X_train = transform_vectorizer(fp_train, sp_train, facts_train, word_vec, facts_vec)\n",
    "    X_test = transform_vectorizer(fp_test, sp_test, facts_test, word_vec, facts_vec)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = word_vectorizer(fp_train, sp_train, facts_train, fp_test, sp_test, facts_test)\n",
    "y_train = train_df['first_party_winner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4956, 207490) (4956,)\n",
      "(1240, 207490)\n",
      "first_party_winner\n",
      "1    2478\n",
      "0    2478\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentated Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape\n",
      "(3717, 207490) (3717,)\n",
      "--------------------\n",
      "Train target\n",
      "first_party_winner\n",
      "1    1859\n",
      "0    1858\n",
      "Name: count, dtype: int64\n",
      "====================\n",
      "Validation Data Shape\n",
      "(1239, 207490) (1239,)\n",
      "--------------------\n",
      "Validation target\n",
      "first_party_winner\n",
      "0    620\n",
      "1    619\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Train, Validation 분리\n",
    "Train_X, Val_X, Train_y, Val_y = train_test_split(X_train, y_train, test_size=.25, random_state=1004, stratify=y_train)\n",
    "print('Train Data Shape')\n",
    "print(Train_X.shape, Train_y.shape)\n",
    "print('-'*20)\n",
    "print('Train target')\n",
    "print(Train_y.value_counts())\n",
    "print('='*20)\n",
    "print('Validation Data Shape')\n",
    "print(Val_X.shape, Val_y.shape)\n",
    "print('-'*20)\n",
    "print('Validation target')\n",
    "print(Val_y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.42      0.42       620\n",
      "           1       0.42      0.42      0.42       619\n",
      "\n",
      "    accuracy                           0.42      1239\n",
      "   macro avg       0.42      0.42      0.42      1239\n",
      "weighted avg       0.42      0.42      0.42      1239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Logistic = LogisticRegression(max_iter=500, random_state=42)\n",
    "Logistic.fit(Train_X, Train_y)\n",
    "print(classification_report(Val_y, Logistic.predict(Val_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['first_party_winner'] = Logistic.predict(X_test)\n",
    "submission_df.to_csv('logi___3.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape after UnderSampling\n",
      "(3595, 207490) (3595,)\n",
      "====================\n",
      "Train target after UnderSampling\n",
      "first_party_winner\n",
      "0    2478\n",
      "1    1117\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불균형 문제 전처리(언더샘플링)\n",
    "X_nc, y_nc = NeighbourhoodCleaningRule(n_neighbors=3).fit_resample(X_train, y_train)\n",
    "print('Train Data Shape after UnderSampling')\n",
    "print(X_nc.shape, y_nc.shape)\n",
    "print('='*20)\n",
    "print('Train target after UnderSampling')\n",
    "print(y_nc.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape\n",
      "(2696, 207490) (2696,)\n",
      "--------------------\n",
      "Train target\n",
      "first_party_winner\n",
      "0    1858\n",
      "1     838\n",
      "Name: count, dtype: int64\n",
      "====================\n",
      "Validation Data Shape\n",
      "(899, 207490) (899,)\n",
      "--------------------\n",
      "Validation target\n",
      "first_party_winner\n",
      "0    620\n",
      "1    279\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Train, Validation 분리\n",
    "Train_X, Val_X, Train_y, Val_y = train_test_split(X_nc, y_nc, test_size=.25, random_state=42, stratify=y_nc)\n",
    "print('Train Data Shape')\n",
    "print(Train_X.shape, Train_y.shape)\n",
    "print('-'*20)\n",
    "print('Train target')\n",
    "print(Train_y.value_counts())\n",
    "print('='*20)\n",
    "print('Validation Data Shape')\n",
    "print(Val_X.shape, Val_y.shape)\n",
    "print('-'*20)\n",
    "print('Validation target')\n",
    "print(Val_y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.90      0.81       620\n",
      "           1       0.54      0.28      0.37       279\n",
      "\n",
      "    accuracy                           0.70       899\n",
      "   macro avg       0.64      0.59      0.59       899\n",
      "weighted avg       0.67      0.70      0.67       899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Logistic = LogisticRegression(max_iter=500, random_state=42)\n",
    "Logistic.fit(Train_X, Train_y)\n",
    "print(classification_report(Val_y, Logistic.predict(Val_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['first_party_winner'] = Logistic.predict(X_test)\n",
    "submission_df.to_csv('logi___2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
