{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc\n",
    "from spacy.language import Language\n",
    "from spacy.tokens.span import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\") # large 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../Data/train.csv').drop('ID', axis=1)\n",
    "test_df = pd.read_csv('../Data/test.csv').drop('ID', axis=1)\n",
    "submission_df = pd.read_csv('../Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.no_possesive(doc)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Language.component(\"no_possesive\") # no_possesive라는 이름의 컴포넌트를 정의\n",
    "\n",
    "def no_possesive(doc):\n",
    "    doc.ents = _no_possesive_generator(doc) # _no_possesive_generator 함수를 통해 doc.ents를 업데이트\n",
    "    return doc\n",
    "\n",
    "def _no_possesive_generator(doc):\n",
    "    \"\"\"Yields non possessive versions of the given document's entities.\"\"\"\n",
    "    for ent in doc.ents:\n",
    "        if ent.text.endswith(\"'s\") or ent.text.endswith(\"’s\"): # 's, ’s로 끝나는 경우\n",
    "            yield Span(doc, ent.start, ent.end-1, label=ent.label) # 끝에 있는 's, ’s를 제거\n",
    "        else:\n",
    "            yield ent\n",
    "\n",
    "# spacy pipeline에 추가\n",
    "nlp.add_pipe(\"no_possesive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_from_span(span, indexes):\n",
    "    \"\"\"\n",
    "    spacy nlp document로부터 특정 index의 token을 제거한 span을 반환하는 함수\n",
    "    \"\"\"\n",
    "    nlp_list = np.array(span) # span을 np.array로 변환\n",
    "    nlp_list = np.delete(nlp_list, indexes).tolist() # indexes에 해당하는 단어를 제거\n",
    "    return nlp(\" \".join([e.text for e in nlp_list])) # 제거한 단어를 제외한 나머지 단어를 join\n",
    "\n",
    "    \n",
    "def extract_dots_pos_nlp(nlp_words, search_pattern=\"\\.\"):\n",
    "    \"\"\"\n",
    "    entity에 포함시키기 어려운 단어(st. Mr. 등)을 제거하는 함수\n",
    "    \"\"\"\n",
    "    pos = []\n",
    "    pattern = re.compile(search_pattern) # search_pattern에 해당하는 패턴을 정규표현식으로 컴파일\n",
    "    for i, word in enumerate(nlp_words):\n",
    "        if bool(pattern.search(word.text)): # search_pattern에 해당하는 단어가 있는 경우\n",
    "            pos.append(i) # 해당 단어의 인덱스를 pos에 추가\n",
    "    return pos\n",
    "\n",
    "def merge_phrases(matches, doc_len):\n",
    "    \"\"\"\n",
    "    연속된 구간의 pharse를 리스트의 인덱스로 찾아서 하나의 entity로 만드는 함수\n",
    "    \"\"\"\n",
    "    def consecutive(data, stepsize=1): # 연속된 숫자를 찾는 함수\n",
    "        return np.split(data, np.where(np.diff(data) != stepsize)[0]+1) # 연속된 숫자를 찾아서 split\n",
    "\n",
    "    match_mask = np.zeros(doc_len) # 문장의 길이만큼 0으로 채워진 배열 생성\n",
    "    for match_id, start, end in matches:\n",
    "        match_mask[start:end+1] = 1 # match되는 부분을 1로 변경\n",
    "    new_matches = consecutive(np.where(match_mask==1)[0]) # 연속된 숫자를 찾아서 split\n",
    "    new_matches = [[idx, match[0], match[-1]] for idx, match in enumerate(new_matches)] # 연속된 숫자의 시작과 끝을 저장\n",
    "    return new_matches\n",
    "\n",
    "def extract_named_entities(doc, entity_types=[\"PERSON\", \"ORG\", \"GPE\"]):\n",
    "    \"\"\"\n",
    "    nlp document로부터 Named Entity를 추출하는 함수\n",
    "    \"\"\"\n",
    "    entities = [(ent.text, ent.label_, ent.ent_id_) for ent in doc.ents] # entity의 text, label, id를 저장\n",
    "    target_entities = []\n",
    "    for ent in entities: # entity_types에 해당하는 entity를 저장\n",
    "        if ent[1] in [\"PERSON\", \"ORG\", \"GPE\"]:\n",
    "            target_entities.append(ent[0])\n",
    "    target_entities = np.unique(target_entities) # 중복된 entity를 제거\n",
    "    target_entities = target_entities.tolist() \n",
    "    target_entities = [nlp.make_doc(t) for t in target_entities] # entity를 nlp document로 변환\n",
    "    return target_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "titles = [\"first_party\", \"second_party\"]\n",
    "\n",
    "def get_matched_entities(fact, first, second, verbose=False):\n",
    "    doc = fact # spacy nlp document\n",
    "\n",
    "    # Named Entity를 추출    \n",
    "    entities = extract_named_entities(doc)\n",
    "    if verbose:\n",
    "        print(f\"entities:\\n{entities}\")\n",
    "\n",
    "    # Named Entity를 Phrase Matcher에 추가\n",
    "    terms_segmented_list = {t: [] for t in titles}\n",
    "    terms = [first, second]\n",
    "    for title, term in zip(titles, terms):\n",
    "        new_term = term.replace(\"et al.\", \"\").replace(\"et al\", \"\").strip() # et al 제거\n",
    "        new_term = nlp.make_doc(new_term) # term을 nlp document로 변환\n",
    "\n",
    "        terms_segmented_list[title].append(new_term)\n",
    "\n",
    "        pos_list = extract_dots_pos_nlp(new_term) # 마침표 포함된 토큰의 위치 추출\n",
    "        new_term = remove_words_from_span(new_term, pos_list) # term에서 st. Mr. 등을 제거\n",
    "        terms_segmented_list[title].append(new_term)\n",
    "\n",
    "        # term을 단어 단위로 분리\n",
    "        for i in range(1, len(new_term)):\n",
    "            terms_segmented_list[title].append(new_term[:i])\n",
    "            terms_segmented_list[title].append(new_term[-i:])\n",
    "    if verbose:\n",
    "        for title in titles:\n",
    "            print(f\"phrases({title}): {terms_segmented_list[title]}\")\n",
    "    \n",
    "    # Phrase Matcher를 통해 Named Entity를 문장에서 추출\n",
    "    thresholds = [0.95, 0.8, 0.6] # 탐색 phrase의 길이에 따라 다른 threhold를 적용 시도 : 한 단어짜리 phrase는 tight하게 similarity를 비교\n",
    "    targets = {t: [] for t in titles}\n",
    "\n",
    "    for cur_ent in entities:\n",
    "        for title, terms in terms_segmented_list.items():\n",
    "            skip_flag = False\n",
    "            for term in terms:\n",
    "                if len(term) == 1: # 단어가 1개인 경우\n",
    "                    thres = thresholds[0]\n",
    "                elif len(term) < 4: # 단어가 4개 미만인 경우\n",
    "                    thres = thresholds[1]\n",
    "                else: # 단어가 4개 이상인 경우\n",
    "                    thres = thresholds[2]\n",
    "                sim = cur_ent.similarity(term) # 두 단어의 유사도 계산\n",
    "                if verbose: \n",
    "                    print(f\"\\t{cur_ent} vs {term} : {sim}\")\n",
    "                if sim > thres: # 유사도가 thres보다 큰 경우\n",
    "                    targets[title].append(cur_ent) # targets에 추가\n",
    "                    skip_flag = True # skip_flag를 True로 변경\n",
    "                    break\n",
    "            if skip_flag:\n",
    "                continue\n",
    "    if verbose:\n",
    "        for title in titles:\n",
    "            print(f\"filtered({title}): {targets[title]}\")\n",
    "    \n",
    "    # 문장에서의 Named Entity 위치 찾기\n",
    "    result = {t: None for t in titles}\n",
    "    for title, data in targets.items():\n",
    "        matcher = PhraseMatcher(nlp.vocab)\n",
    "        matcher.add(\"TerminologyList\", data)\n",
    "\n",
    "        matches = matcher(doc)\n",
    "        if len(matches) == 0:\n",
    "            if verbose:\n",
    "                print(f\"{title} no match found..\")\n",
    "            continue\n",
    "        matches = merge_phrases(matches, len(doc))\n",
    "        result[title] = matches\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_entities(fact_nlp, matches_dict:dict, replaces={\"first_party\": \"[FIRST_PARTY] \", \"second_party\": \"[SECOND_PARTY] \"}):\n",
    "    \"\"\"\n",
    "    문장에서 Named Entity를 대체하는 함수\n",
    "    \"\"\"\n",
    "    new_fact = []\n",
    "    total_len = len(fact_nlp)\n",
    "\n",
    "    def replace_match(cur_idx, start, end, match_idx, match_info, replace_str):\n",
    "        if cur_idx == start:\n",
    "            new_fact.append(replace_str)\n",
    "        elif cur_idx == end:\n",
    "            match_idx += 1\n",
    "            if match_idx < len(match_info):\n",
    "                _, start, end = match_info[match_idx]\n",
    "            else:\n",
    "                # finish matching\n",
    "                start, end = total_len, total_len\n",
    "        else: # skip\n",
    "            pass\n",
    "        return start, end, match_idx\n",
    "\n",
    "    match_idx_first = 0\n",
    "    match_idx_second = 0\n",
    "    matches_first = matches_dict[\"first_party\"]\n",
    "    matches_second = matches_dict[\"second_party\"]\n",
    "\n",
    "    if matches_first is not None:\n",
    "        _, start_first, end_first = matches_first[match_idx_first] # init\n",
    "    else:\n",
    "        start_first, end_first = total_len, total_len\n",
    "\n",
    "    if matches_second is not None:\n",
    "        _, start_second, end_second = matches_second[match_idx_second] # init\n",
    "    else:\n",
    "        start_second, end_second = total_len, total_len\n",
    "\n",
    "    for idx in range(total_len):\n",
    "        if idx >= start_first and idx <= end_first:\n",
    "            start_first, end_first, match_idx_first = replace_match(idx, start_first, end_first, match_idx_first, matches_first, replaces[\"first_party\"])\n",
    "        elif idx >= start_second and idx <= end_second:\n",
    "            start_second, end_second, match_idx_second = replace_match(idx, start_second, end_second, match_idx_second, matches_second, replaces[\"second_party\"])\n",
    "        else:\n",
    "            new_fact.append(fact_nlp[idx].text_with_ws)\n",
    "\n",
    "    new_fact = \"\".join(new_fact).strip()\n",
    "\n",
    "    return new_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas row마다 적용할 최종 wrapper function\n",
    "\n",
    "replaces={\"first_party\": \"[FIRST_PARTY] \", \"second_party\": \"[SECOND_PARTY] \"}\n",
    "\n",
    "def df_row_process(row):\n",
    "    first_party = row[\"first_party\"]\n",
    "    second_party = row[\"second_party\"]\n",
    "\n",
    "    doc = row[\"facts_nlp\"]\n",
    "    match_result = get_matched_entities(doc, first_party, second_party, verbose=False)\n",
    "    new_first = replaces[\"first_party\"].strip() if match_result[\"first_party\"] is not None else first_party\n",
    "    new_second = replaces[\"second_party\"].strip() if match_result[\"second_party\"] is not None else second_party\n",
    "    new_fact = replace_entities(doc, match_result, replaces)\n",
    "\n",
    "    return new_fact, new_first, new_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3946725/3698349475.py:47: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  sim = cur_ent.similarity(term) # 두 단어의 유사도 계산\n"
     ]
    }
   ],
   "source": [
    "# spacy nlp로 변환\n",
    "train_df[\"facts_nlp\"] = train_df['facts'].apply(lambda x : nlp(x))\n",
    "train_df[[\"new_facts\", \"first_party\", \"second_party\"]] = train_df.apply(df_row_process, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "      <th>facts_nlp</th>\n",
       "      <th>new_facts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[FIRST_PARTY]</td>\n",
       "      <td>[SECOND_PARTY]</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>1</td>\n",
       "      <td>(On, June, 27, ,, 1962, ,, Phil, St., Amant, ,...</td>\n",
       "      <td>On June 27, 1962, [FIRST_PARTY] a candidate fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>[SECOND_PARTY]</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Ramon, Nelson, was, riding, his, bike, when, ...</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[FIRST_PARTY]</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>1</td>\n",
       "      <td>(An, Alabama, state, court, convicted, Billy, ...</td>\n",
       "      <td>An Alabama state court convicted [FIRST_PARTY]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Victor, Linkletter, was, convicted, in, state...</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[FIRST_PARTY]</td>\n",
       "      <td>[SECOND_PARTY]</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>1</td>\n",
       "      <td>(On, April, 24, ,, 1953, in, Selma, ,, Alabama...</td>\n",
       "      <td>On April 24, 1953 in Selma, [SECOND_PARTY] an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>HollyFrontier Cheyenne Refining, LLC, et al.</td>\n",
       "      <td>[SECOND_PARTY]</td>\n",
       "      <td>Congress amended the Clean Air Act through the...</td>\n",
       "      <td>1</td>\n",
       "      <td>(Congress, amended, the, Clean, Air, Act, thro...</td>\n",
       "      <td>Congress amended the Clean Air Act through the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>[FIRST_PARTY]</td>\n",
       "      <td>[SECOND_PARTY]</td>\n",
       "      <td>Alliance Bond Fund, Inc., an investment fund, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>(Alliance, Bond, Fund, ,, Inc., ,, an, investm...</td>\n",
       "      <td>[SECOND_PARTY] an investment fund, purchased a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>[FIRST_PARTY]</td>\n",
       "      <td>United States</td>\n",
       "      <td>In 1992, the District Court sentenced Manuel D...</td>\n",
       "      <td>0</td>\n",
       "      <td>(In, 1992, ,, the, District, Court, sentenced,...</td>\n",
       "      <td>In 1992, the District Court sentenced Manuel D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>[FIRST_PARTY]</td>\n",
       "      <td>[SECOND_PARTY]</td>\n",
       "      <td>On March 8, 1996, Enrico St. Cyr, a lawful per...</td>\n",
       "      <td>0</td>\n",
       "      <td>(On, March, 8, ,, 1996, ,, Enrico, St., Cyr, ,...</td>\n",
       "      <td>On March 8, 1996, [SECOND_PARTY] a lawful perm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>[FIRST_PARTY]</td>\n",
       "      <td>[SECOND_PARTY]</td>\n",
       "      <td>Herbert Markman owns the patent to a system th...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Herbert, Markman, owns, the, patent, to, a, s...</td>\n",
       "      <td>Herbert [FIRST_PARTY] the patent to a system t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2478 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       first_party   \n",
       "0                                    [FIRST_PARTY]  \\\n",
       "1                                   Stephen Duncan   \n",
       "2                                    [FIRST_PARTY]   \n",
       "3                                       Linkletter   \n",
       "4                                    [FIRST_PARTY]   \n",
       "...                                            ...   \n",
       "2473  HollyFrontier Cheyenne Refining, LLC, et al.   \n",
       "2474                                 [FIRST_PARTY]   \n",
       "2475                                 [FIRST_PARTY]   \n",
       "2476                                 [FIRST_PARTY]   \n",
       "2477                                 [FIRST_PARTY]   \n",
       "\n",
       "                        second_party   \n",
       "0                     [SECOND_PARTY]  \\\n",
       "1                     [SECOND_PARTY]   \n",
       "2     Tony Patterson, Warden, et al.   \n",
       "3                             Walker   \n",
       "4                     [SECOND_PARTY]   \n",
       "...                              ...   \n",
       "2473                  [SECOND_PARTY]   \n",
       "2474                  [SECOND_PARTY]   \n",
       "2475                   United States   \n",
       "2476                  [SECOND_PARTY]   \n",
       "2477                  [SECOND_PARTY]   \n",
       "\n",
       "                                                  facts  first_party_winner   \n",
       "0     On June 27, 1962, Phil St. Amant, a candidate ...                   1  \\\n",
       "1     Ramon Nelson was riding his bike when he suffe...                   0   \n",
       "2     An Alabama state court convicted Billy Joe Mag...                   1   \n",
       "3     Victor Linkletter was convicted in state court...                   0   \n",
       "4     On April 24, 1953 in Selma, Alabama, an intrud...                   1   \n",
       "...                                                 ...                 ...   \n",
       "2473  Congress amended the Clean Air Act through the...                   1   \n",
       "2474  Alliance Bond Fund, Inc., an investment fund, ...                   1   \n",
       "2475  In 1992, the District Court sentenced Manuel D...                   0   \n",
       "2476  On March 8, 1996, Enrico St. Cyr, a lawful per...                   0   \n",
       "2477  Herbert Markman owns the patent to a system th...                   0   \n",
       "\n",
       "                                              facts_nlp   \n",
       "0     (On, June, 27, ,, 1962, ,, Phil, St., Amant, ,...  \\\n",
       "1     (Ramon, Nelson, was, riding, his, bike, when, ...   \n",
       "2     (An, Alabama, state, court, convicted, Billy, ...   \n",
       "3     (Victor, Linkletter, was, convicted, in, state...   \n",
       "4     (On, April, 24, ,, 1953, in, Selma, ,, Alabama...   \n",
       "...                                                 ...   \n",
       "2473  (Congress, amended, the, Clean, Air, Act, thro...   \n",
       "2474  (Alliance, Bond, Fund, ,, Inc., ,, an, investm...   \n",
       "2475  (In, 1992, ,, the, District, Court, sentenced,...   \n",
       "2476  (On, March, 8, ,, 1996, ,, Enrico, St., Cyr, ,...   \n",
       "2477  (Herbert, Markman, owns, the, patent, to, a, s...   \n",
       "\n",
       "                                              new_facts  \n",
       "0     On June 27, 1962, [FIRST_PARTY] a candidate fo...  \n",
       "1     Ramon Nelson was riding his bike when he suffe...  \n",
       "2     An Alabama state court convicted [FIRST_PARTY]...  \n",
       "3     Victor Linkletter was convicted in state court...  \n",
       "4     On April 24, 1953 in Selma, [SECOND_PARTY] an ...  \n",
       "...                                                 ...  \n",
       "2473  Congress amended the Clean Air Act through the...  \n",
       "2474  [SECOND_PARTY] an investment fund, purchased a...  \n",
       "2475  In 1992, the District Court sentenced Manuel D...  \n",
       "2476  On March 8, 1996, [SECOND_PARTY] a lawful perm...  \n",
       "2477  Herbert [FIRST_PARTY] the patent to a system t...  \n",
       "\n",
       "[2478 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = pd.read_csv('../Data/train.csv').drop('ID', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carol Anne Bond\n",
      "United States\n",
      "Carol Anne Bond was found guilty of trying to poison her husband's mistress, Myrlinda Haynes, with toxic chemicals at least 24 times over the course of several months. A grand jury in the Eastern District of Pennsylvania charged Bond with two counts of possessing and using a chemical weapon, in violation of a criminal statute implementing the treaty obligations of the United States under the 1993 Chemical Weapons Convention. The grand jury also charged Bond with two counts of mail theft. Bond's attorneys argue that the statute was intended to deal with rogue states and terrorists and that their client should have been prosecuted under state law instead. Bond, a laboratory technician, stole the chemical potassium dichromate from the company where she worked. Haynes was not injured. Bond's husband had a child with Haynes while married to Bond. Haynes had contacted police and postal authorities after finding the chemicals at her home. In September 2009, the U.S. Court of Appeals for the Third Circuit held that Bond lacked standing to challenge the constitutionality of the statute on the basis of the Tenth Amendment.\n",
      "\n",
      "[FIRST_PARTY] found guilty of trying to poison her husband's mistress, Myrlinda Haynes, with toxic chemicals at least 24 times over the course of several months. A grand jury in the Eastern District of Pennsylvania charged [FIRST_PARTY] two counts of possessing and using a chemical weapon, in violation of a criminal statute implementing the treaty obligations of [SECOND_PARTY] the 1993 Chemical Weapons Convention. The grand jury also charged [FIRST_PARTY] two counts of mail theft. [FIRST_PARTY] attorneys argue that the statute was intended to deal with rogue states and terrorists and that their client should have been prosecuted under state law instead. [FIRST_PARTY] a laboratory technician, stole the chemical potassium dichromate from the company where she worked. Haynes was not injured. [FIRST_PARTY] husband had a child with Haynes while married to [FIRST_PARTY] Haynes had contacted police and postal authorities after finding the chemicals at her home. In September 2009, the U.S. Court of Appeals for the Third Circuit held that [FIRST_PARTY] standing to challenge the constitutionality of the statute on the basis of the Tenth Amendment.\n"
     ]
    }
   ],
   "source": [
    "idx = 1234\n",
    "print(train_df_1['first_party'][idx])\n",
    "print(train_df_1['second_party'][idx])\n",
    "print(train_df['facts'][idx])\n",
    "print(train_df['new_facts'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
